{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alloc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alloc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**importing pakages**\n",
      "**Loading data**\n",
      "**Preprocessing data**\n",
      "**Spliting traing and validation data**\n"
     ]
    }
   ],
   "source": [
    "print('**importing pakages**')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "\n",
    "print('**Loading data**')\n",
    "df = pd.read_csv('./reddit_train.csv')\n",
    "\n",
    "print('**Preprocessing data**')\n",
    "df['clean_comments'] = df['comments'].replace(r'http\\S', '', regex=True).replace(r'www\\S', '', regex=True)\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"[^0-9a-zA-Z#+_]\", \" \")\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"&(\\w);\", \" \")\n",
    "#df['clean_comments'] = df['clean_comments'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "tokenized_comments = df['clean_comments'].apply(lambda x: x.split())\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [stemmer.stem(word)for word in x])\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [lem.lemmatize(word) for word in x])\n",
    "for i in range(len(tokenized_comments)):\n",
    "    tokenized_comments[i] = ' '.join(tokenized_comments[i])\n",
    "\n",
    "df['clean_comments'] = tokenized_comments\n",
    "\n",
    "\n",
    "# count_vect = CountVectorizer( lowercase=True, preprocessor=None, tokenizer=None,\n",
    "#                              stop_words='english',  ngram_range=(1,2),\n",
    "#                              analyzer='word', max_df=0.5, min_df=3, max_features=20000, vocabulary=None,\n",
    "#                              binary=True)\n",
    "# count = count_vect.fit_transform(df['clean_comments'])\n",
    "# train_data = count\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.99, min_df=2, smooth_idf = False, norm='l2', ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['clean_comments'])\n",
    "train_data = tfidf\n",
    "\n",
    "\n",
    "LE = LabelEncoder()\n",
    "df['label'] = LE.fit_transform(df['subreddits'])\n",
    "train_label = df['label'].to_numpy()\n",
    "\n",
    "print('**Spliting traing and validation data**')\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_label, test_size = 0.2, random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OnevsRest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('**Fitting model**')\n",
    "vs =OneVsRestClassifier(clf, n_jobs=-1,verbose=True)\n",
    "vs.fit(X_train, y_train)\n",
    "y_predict = vs.predict(X_valid)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_valid, y_predict)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fitting model**\n",
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alloc\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.065\n"
     ]
    }
   ],
   "source": [
    "print('**Fitting model**')\n",
    "clf = LinearSVC(penalty='l2', loss='hinge', dual=True, tol=0.01, C=5000,\n",
    "                multi_class='ovr', fit_intercept=False, intercept_scaling=10,\n",
    "                class_weight=None, verbose=10, random_state=42, max_iter=100)\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_valid)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_valid, y_predict)\n",
    "print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_pipeline = Pipeline([('clf', LinearSVC(verbose=True))])\n",
    "parameters_clf = {\n",
    "                  'clf__tol': (1e-4,1e-5,1e-3),\n",
    "                  'clf__C':(0.2,0.1,0.05),\n",
    "                  'clf__dual':(True,False),\n",
    "                   'clf__intercept_scaling':(2,5,10,20),\n",
    "    \n",
    "                      \n",
    " }\n",
    "gs_clf_svm = GridSearchCV(clf_pipeline, parameters_clf, n_jobs=-1,verbose=10)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('**Fitting model**')\n",
    "\n",
    "clf_dtc = DecisionTreeClassifier(criterion='gini', splitter='best',\n",
    "                                 max_depth=None, min_samples_split=10, min_samples_leaf=15,\n",
    "                                 min_weight_fraction_leaf=0.0, max_features=None, \n",
    "                                 random_state=None, max_leaf_nodes=None,\n",
    "                                 min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n",
    "clf_dtc= clf_dtc.fit(X_train, y_train)\n",
    "y_predict = clf_dtc.predict(X_valid)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_valid, y_predict)\n",
    "print('Accuracy: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Fitting model**\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alloc\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1224\n"
     ]
    }
   ],
   "source": [
    "print('**Fitting model**')\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "clf_sdg = SGDClassifier(loss='modified_huber', penalty='l2', alpha=0.001,\n",
    "                        l1_ratio=0.15, fit_intercept=True, max_iter=100, tol=0.00005, shuffle=True, verbose=0,\n",
    "                        epsilon=1, n_jobs=-1, random_state=True, learning_rate='optimal', eta0=1.0, power_t=0.5,\n",
    "                        early_stopping=False,\n",
    "                        validation_fraction=0.9, n_iter_no_change=70, class_weight=None, warm_start=True, average=False)\n",
    "clf_sdg= clf_sdg.fit(X_train, y_train)\n",
    "y_predict = clf_sdg.predict(X_valid)\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_valid, y_predict)\n",
    "print('Accuracy: {:.4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_pipeline = Pipeline([('clf', SGDClassifier( eta0=1.0))])\n",
    "parameters_clf = {\n",
    "                  'clf__alpha': (1e-4,7e-5,4e-4),\n",
    "                  'clf__tol':(0.0001,0.00001,0.000001),\n",
    "                 'clf__learning_rate':('optimal','adaptive','invscaling'),\n",
    "                    'clf__average':(True,False),\n",
    "    \n",
    "                      \n",
    " }\n",
    "gs_clf_svm = GridSearchCV(clf_pipeline, parameters_clf, n_jobs=-1,verbose=10)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5734285714285714"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "clf = MultinomialNB(alpha=0.27, fit_prior=False, class_prior=None)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_valid)\n",
    "np.mean(predicted == y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "clf_pipeline = Pipeline([('clf', MultinomialNB())])\n",
    "parameters_clf = {\n",
    "                  'clf__alpha': (0.7,0.35,0.45,0.4),\n",
    "                  'clf__fit_prior':(True,False),\n",
    "                  ## 'clf__class_prior':(True,False),\n",
    "    \n",
    "                      \n",
    " }\n",
    "gs_clf_svm = GridSearchCV(clf_pipeline, parameters_clf, n_jobs=-1,verbose=10)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best MAXDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxDF = 0.01\n",
    "best_maxDF=0.0\n",
    "accuracy = 0.0\n",
    "while maxDF <0.9:\n",
    "    ##setting up tfidf with maxDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_df=maxDF, min_df=2, smooth_idf = True, norm='l2', ngram_range=(1, 5), stop_words='english')\n",
    "    # TF-IDF feature matrix\n",
    "    tfidf = tfidf_vectorizer.fit_transform(df['clean_comments'])\n",
    "    train_data = tfidf\n",
    "    LE = LabelEncoder()\n",
    "    df['label'] = LE.fit_transform(df['subreddits'])\n",
    "    train_label = df['label'].to_numpy()\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_label, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "   \n",
    "    clf_sdg = SGDClassifier(loss='modified_huber', penalty='l2', alpha=0.0001,\n",
    "                        l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.0001, shuffle=True, verbose=0,\n",
    "                        epsilon=5, n_jobs=-1, random_state=None, learning_rate='optimal', eta0=1.0, power_t=0.5,\n",
    "                        early_stopping=False,\n",
    "                        validation_fraction=0.8, n_iter_no_change=50, class_weight=None, warm_start=False, average=False)\n",
    "    clf_sdg= clf_sdg.fit(X_train, y_train)\n",
    "    y_predict = clf_sdg.predict(X_valid)\n",
    "    \n",
    "    if (accuracy < metrics.accuracy_score(y_valid, y_predict)):\n",
    "        accuracy = metrics.accuracy_score(y_valid, y_predict)\n",
    "        best_maxDF=maxDF\n",
    "    \n",
    "    maxDF = maxDF + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy)\n",
    "print(best_maxDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_maxDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from gensim.models import Doc2Vec\n",
    "from sklearn import utils\n",
    "import gensim\n",
    "from gensim.models import doc2vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "\n",
    "def label_sentences(corpus, label_type):\n",
    "    \"\"\"\n",
    "    Gensim's Doc2Vec implementation requires each document/paragraph to have a label associated with it.\n",
    "    We do this by using the TaggedDocument method. The format will be \"TRAIN_i\" or \"TEST_i\" where \"i\" is\n",
    "    a dummy index of the post.\n",
    "    \"\"\"\n",
    "    labeled = []\n",
    "    for i, v in enumerate(corpus):\n",
    "        label = label_type + '_' + str(i)\n",
    "        labeled.append(doc2vec.TaggedDocument(v, [label]))\n",
    "    return labeled\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(df['clean_comments'], df['label'], random_state=0, test_size=0.3)\n",
    "X_train = label_sentences(X_train, 'Train')\n",
    "X_valid = label_sentences(X_valid, 'Test')\n",
    "all_data = X_train + X_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4379559.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3898517.88it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4381912.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4387019.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4386560.69it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3338958.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4379232.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 1462755.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4129355.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3338806.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4128658.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3513616.16it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4133075.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4386953.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4670419.95it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3194825.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4129006.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 2064358.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4128716.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4668266.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3899501.67it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4381585.48it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4121934.61it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3340097.84it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3899242.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 2924402.92it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4386888.40it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 3897845.04it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4128658.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4120835.39it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 70000/70000 [00:00<00:00, 4377926.76it/s]\n"
     ]
    }
   ],
   "source": [
    "model_dbow = Doc2Vec(dm=0, vector_size=300, negative=5, min_count=1, alpha=0.065, min_alpha=0.065)\n",
    "model_dbow.build_vocab([x for x in tqdm(all_data)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_dbow.train(utils.shuffle([x for x in tqdm(all_data)]), total_examples=len(all_data), epochs=1)\n",
    "    model_dbow.alpha -= 0.002\n",
    "    model_dbow.min_alpha = model_dbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(model, corpus_size, vectors_size, vectors_type):\n",
    "    \"\"\"\n",
    "    Get vectors from trained doc2vec model\n",
    "    :param doc2vec_model: Trained Doc2Vec model\n",
    "    :param corpus_size: Size of the data\n",
    "    :param vectors_size: Size of the embedding vectors\n",
    "    :param vectors_type: Training or Testing vectors\n",
    "    :return: list of vectors\n",
    "    \"\"\"\n",
    "    vectors = np.zeros((corpus_size, vectors_size))\n",
    "    for i in range(0, corpus_size):\n",
    "        prefix = vectors_type + '_' + str(i)\n",
    "        vectors[i] = model.docvecs[prefix]\n",
    "    return vectors\n",
    "    \n",
    "X_train = get_vectors(model_dbow, len(X_train), 300, 'Train')\n",
    "X_valid = get_vectors(model_dbow, len(X_test), 300, 'Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(n_jobs=-1, C=1e5)\n",
    "logreg.fit(train_vectors_dbow, y_train)\n",
    "logreg = logreg.fit(train_vectors_dbow, y_train)\n",
    "y_pred = logreg.predict(test_vectors_dbow)\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.62307217\n",
      "Iteration 2, loss = 1.70338398\n",
      "Iteration 3, loss = 1.30452745\n",
      "Iteration 4, loss = 1.09678012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alloc\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.586"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(95,), activation='relu', solver='adam',\n",
    "                    alpha=0.00005, batch_size=200, learning_rate='invscaling',\n",
    "                    learning_rate_init=0.0009, power_t=0.9, max_iter=4, shuffle=True,\n",
    "                    random_state=False, tol=0.1, verbose=True, warm_start=False,\n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                    validation_fraction=0.8, beta_1=0.9, beta_2=0.99, epsilon=1e-8,\n",
    "                    n_iter_no_change=1)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_valid)\n",
    "np.mean(predicted == y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam',\n",
    "                    alpha=0.0001, batch_size='auto', learning_rate='adaptive',\n",
    "                    learning_rate_init=0.00, power_t=0.5, max_iter=8, shuffle=True,\n",
    "                    random_state=None, tol=0.0001, verbose=True, warm_start=False,\n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                    validation_fraction=0.3, beta_1=0.8, beta_2=0.99, epsilon=1e-08,\n",
    "                    n_iter_no_change=10)\n",
    "from sklearn.pipeline import Pipeline\n",
    "clf_pipeline = Pipeline([('clf', MLPClassifier(verbose=True,alpha=1e-3))])\n",
    "parameters_clf = {'clf__max_iter': (3,4,5),\n",
    "                  'clf__alpha': (1e-4,1e-3),\n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "                  \n",
    "               \n",
    "               \n",
    " }\n",
    "gs_clf_svm = GridSearchCV(clf_pipeline, parameters_clf, n_jobs=-1,verbose=10)\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train, y_train)\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      139594.0769            4.04m\n",
      "         2      131935.3108            3.68m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-650cd9f5b5b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_leaf_nodes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpresort\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                 n_iter_no_change=None, tol=0.001)\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m   1544\u001b[0m         n_stages = self._fit_stages(\n\u001b[0;32m   1545\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1546\u001b[1;33m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[0;32m   1547\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1548\u001b[0m         \u001b[1;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1608\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[0;32m   1609\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1610\u001b[1;33m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[0;32m   1611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1612\u001b[0m             \u001b[1;31m# track deviance (= loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\ensemble\\gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m   1242\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1243\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[1;32m-> 1244\u001b[1;33m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1245\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1246\u001b[0m             \u001b[1;31m# update tree leaves\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1155\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1156\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1157\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m   1158\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf =GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=10,\n",
    "                                subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1,\n",
    "                                min_weight_fraction_leaf=0.0, max_depth=16, min_impurity_decrease=0.0,\n",
    "                                min_impurity_split=None, init=None, random_state=None, max_features=None,\n",
    "                                verbose=10, max_leaf_nodes=None, warm_start=False, presort='auto', validation_fraction=0.1,\n",
    "                                n_iter_no_change=None, tol=0.001)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_valid)\n",
    "np.mean(predicted == y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LogReg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(penalty='l2', dual=False, tol=0.001,\n",
    "                   C=1.0, fit_intercept=True, intercept_scaling=1,\n",
    "                   class_weight=None, random_state=None, solver='newton-cg',\n",
    "                   max_iter=100, multi_class='auto', verbose=10,\n",
    "                   warm_start=False, n_jobs=-1, l1_ratio=None)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_valid)\n",
    "np.mean(predicted == y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=20, criterion='gini', max_depth=None, min_samples_split=3,\n",
    "                             min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, \n",
    "                             bootstrap=True, oob_score=False, n_jobs=-1, random_state=None, verbose=10,\n",
    "                             warm_start=False, class_weight=None)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_valid)\n",
    "np.mean(predicted == y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(clf,data,labels,k):\n",
    "        i=0\n",
    "        score=0\n",
    "        batch_size=np.floor(data[:,1].size/k)\n",
    "        kf = KFold(n_splits=k)\n",
    "        kf.get_n_splits(train_data)\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            training_data= data[train_index]\n",
    "            \n",
    "            training_label=labels[train_index]\n",
    "            \n",
    "            test_data= data[test_index]\n",
    "            test_labels = labels[test_index]\n",
    "            clf= clf.fit(training_data,training_label)\n",
    "            preds = clf.predict(test_data)\n",
    "            print(\"\\nSegmentation #\",i+1,\" Results:\")\n",
    "            print(np.mean(preds == test_labels))\n",
    "            score+=np.mean(preds == test_labels)\n",
    "            i+=1\n",
    "            \n",
    "        \n",
    "        print(\"\\nAverage Accuracy Score: \",score/k,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Segmentation # 1  Results:\n",
      "0.572\n",
      "\n",
      "Segmentation # 2  Results:\n",
      "0.5718571428571428\n",
      "\n",
      "Segmentation # 3  Results:\n",
      "0.577\n",
      "\n",
      "Segmentation # 4  Results:\n",
      "0.5892857142857143\n",
      "\n",
      "Segmentation # 5  Results:\n",
      "0.5675714285714286\n",
      "\n",
      "Segmentation # 6  Results:\n",
      "0.5732857142857143\n",
      "\n",
      "Segmentation # 7  Results:\n",
      "0.5782857142857143\n",
      "\n",
      "Segmentation # 8  Results:\n",
      "0.5708571428571428\n",
      "\n",
      "Segmentation # 9  Results:\n",
      "0.5727142857142857\n",
      "\n",
      "Segmentation # 10  Results:\n",
      "0.5804285714285714\n",
      "\n",
      "Average Accuracy Score:  0.5753285714285715 %\n"
     ]
    }
   ],
   "source": [
    "kfold(clf,train_data,train_label,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST DATA PREDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Loading data**\n",
      "**Preprocessing data**\n"
     ]
    }
   ],
   "source": [
    "print('**Loading data**')\n",
    "df = pd.read_csv('./reddit_test.csv')\n",
    "\n",
    "print('**Preprocessing data**')\n",
    "df['clean_comments'] = df['comments'].replace('http\\S', '', regex=True).replace('www\\S', '', regex=True)\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"[^0-9a-zA-Z#+_]\", \" \")\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"&(\\w);\", \" \")\n",
    "#df['clean_comments'] = df['clean_comments'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "tokenized_comments = df['clean_comments'].apply(lambda x: x.split())\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [stemmer.stem(word)for word in x])\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [lem.lemmatize(word) for word in x])\n",
    "for i in range(len(tokenized_comments)):\n",
    "    tokenized_comments[i] = ' '.join(tokenized_comments[i])\n",
    "\n",
    "df['clean_comments'] = tokenized_comments\n",
    "\n",
    "\n",
    "\n",
    "#tfidf_vectorizer = TfidfVectorizer(max_df=0.99, min_df=2, smooth_idf = True, norm='l2', ngram_range=(1, 1), stop_words='english')\n",
    "# TF-IDF feature matrix\n",
    "tfidf = tfidf_vectorizer.transform(df['clean_comments'])\n",
    "test_data = tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 24567)\n",
      "Iteration 1, loss = 2.49909279\n",
      "Iteration 2, loss = 1.56542284\n",
      "Iteration 3, loss = 1.24019864\n",
      "Iteration 4, loss = 1.06411132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alloc\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print(test_data.shape)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(95,), activation='relu', solver='adam',\n",
    "                    alpha=0.00005, batch_size=200, learning_rate='invscaling',\n",
    "                    learning_rate_init=0.0009, power_t=0.9, max_iter=4, shuffle=True,\n",
    "                    random_state=False, tol=0.1, verbose=True, warm_start=False,\n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                    validation_fraction=0.8, beta_1=0.9, beta_2=0.99, epsilon=1e-8,\n",
    "                    n_iter_no_change=1)\n",
    "clf = clf.fit(train_data, train_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= clf.predict(test_data)\n",
    "np.savetxt(\"predictions.csv\", np.vstack((df['id'], LE.inverse_transform(res))).T, delimiter=\",\",fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
