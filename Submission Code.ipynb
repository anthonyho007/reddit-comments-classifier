{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alloc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alloc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**importing pakages**\n",
      "**Loading data**\n",
      "**Preprocessing data**\n",
      "**Spliting traing and validation data**\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "print('**importing pakages**')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn import linear_model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import LinearSVC \n",
    "\n",
    "\n",
    "print('**Loading data**')\n",
    "df = pd.read_csv('./reddit_train.csv')\n",
    "\n",
    "print('**Preprocessing data**')\n",
    "df['clean_comments'] = df['comments'].replace(r'http\\S', '', regex=True).replace(r'www\\S', '', regex=True)\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"[^0-9a-zA-Z#+_]\", \" \")\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"&(\\w);\", \" \")\n",
    "#df['clean_comments'] = df['clean_comments'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "tokenized_comments = df['clean_comments'].apply(lambda x: x.split())\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [stemmer.stem(word)for word in x])\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [lem.lemmatize(word) for word in x])\n",
    "for i in range(len(tokenized_comments)):\n",
    "    tokenized_comments[i] = ' '.join(tokenized_comments[i])\n",
    "\n",
    "df['clean_comments'] = tokenized_comments\n",
    "\n",
    "\n",
    "# count_vect = CountVectorizer( lowercase=True, preprocessor=None, tokenizer=None,\n",
    "#                              stop_words='english',  ngram_range=(1,2),\n",
    "#                              analyzer='word', max_df=0.5, min_df=3, max_features=20000, vocabulary=None,\n",
    "#                              binary=True)\n",
    "# count = count_vect.fit_transform(df['clean_comments'])\n",
    "# train_data = count\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.99, min_df=2, smooth_idf = True, norm='l2', ngram_range=(1,1), stop_words='english')\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(df['clean_comments'])\n",
    "train_data = tfidf\n",
    "\n",
    "\n",
    "LE = LabelEncoder()\n",
    "df['label'] = LE.fit_transform(df['subreddits'])\n",
    "train_label = df['label'].to_numpy()\n",
    "\n",
    "print('**Spliting traing and validation data**')\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(train_data, train_label, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 2.62295746\n",
      "Iteration 2, loss = 1.70363562\n",
      "Iteration 3, loss = 1.30565989\n",
      "Iteration 4, loss = 1.09853025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alloc\\Anaconda3\\envs\\comp551a1\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5860714285714286"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(hidden_layer_sizes=(95,), activation='relu', solver='adam',\n",
    "                    alpha=0.00005, batch_size=200, learning_rate='invscaling',\n",
    "                    learning_rate_init=0.0009, power_t=0.9, max_iter=4, shuffle=True,\n",
    "                    random_state=False, tol=0.1, verbose=True, warm_start=False,\n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                    validation_fraction=0.8, beta_1=0.9, beta_2=0.99, epsilon=1e-8,\n",
    "                    n_iter_no_change=1)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predicted = clf.predict(X_valid)\n",
    "np.mean(predicted == y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kfold(clf,data,labels,k):\n",
    "        i=0\n",
    "        score=0\n",
    "        batch_size=np.floor(data[:,1].size/k)\n",
    "        kf = KFold(n_splits=k)\n",
    "        kf.get_n_splits(train_data)\n",
    "        for train_index, test_index in kf.split(data):\n",
    "            training_data= data[train_index]\n",
    "            \n",
    "            training_label=labels[train_index]\n",
    "            \n",
    "            test_data= data[test_index]\n",
    "            test_labels = labels[test_index]\n",
    "            clf= clf.fit(training_data,training_label)\n",
    "            preds = clf.predict(test_data)\n",
    "            print(\"\\nSegmentation #\",i+1,\" Results:\")\n",
    "            print(np.mean(preds == test_labels))\n",
    "            score+=np.mean(preds == test_labels)\n",
    "            i+=1\n",
    "            \n",
    "        \n",
    "        print(\"\\nAverage Accuracy Score: \",score/k,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold(clf,train_data,train_label,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('**Loading data**')\n",
    "df = pd.read_csv('./reddit_test.csv')\n",
    "\n",
    "print('**Preprocessing data**')\n",
    "df['clean_comments'] = df['comments'].replace('http\\S', '', regex=True).replace('www\\S', '', regex=True)\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"[^0-9a-zA-Z#+_]\", \" \")\n",
    "df['clean_comments'] = df['clean_comments'].str.replace(\"&(\\w);\", \" \")\n",
    "#df['clean_comments'] = df['clean_comments'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))\n",
    "tokenized_comments = df['clean_comments'].apply(lambda x: x.split())\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [stemmer.stem(word)for word in x])\n",
    "\n",
    "lem = WordNetLemmatizer()\n",
    "tokenized_comments = tokenized_comments.apply(lambda x: [lem.lemmatize(word) for word in x])\n",
    "for i in range(len(tokenized_comments)):\n",
    "    tokenized_comments[i] = ' '.join(tokenized_comments[i])\n",
    "\n",
    "df['clean_comments'] = tokenized_comments\n",
    "\n",
    "\n",
    "\n",
    "tfidf = tfidf_vectorizer.transform(df['clean_comments'])\n",
    "test_data = tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_data.shape)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(95,), activation='relu', solver='adam',\n",
    "                    alpha=0.00005, batch_size=200, learning_rate='invscaling',\n",
    "                    learning_rate_init=0.0009, power_t=0.9, max_iter=4, shuffle=True,\n",
    "                    random_state=False, tol=0.1, verbose=True, warm_start=False,\n",
    "                    momentum=0.9, nesterovs_momentum=True, early_stopping=False,\n",
    "                    validation_fraction=0.8, beta_1=0.9, beta_2=0.99, epsilon=1e-8,\n",
    "                    n_iter_no_change=1)\n",
    "clf = clf.fit(train_data, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res= clf.predict(test_data)\n",
    "np.savetxt(\"predictions.csv\", np.vstack((df['id'], LE.inverse_transform(res))).T, delimiter=\",\",fmt='%s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
